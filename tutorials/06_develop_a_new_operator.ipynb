{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e3ddc2c-7871-47ee-ad42-02676bf0451e",
   "metadata": {},
   "source": [
    "# Develop a New Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding for Your Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add a new StatsKeys in `data_juicer/utils/constant.py` to store the statistical variable of the new operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatsKeys(object):\n",
    "    # ... other keys\n",
    "    text_len = 'text_len'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a new operator file, such as `your_text_length_filter.py` in the corresponding `data_juicer/ops/filter/` directory as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from jsonargparse.typing import PositiveInt\n",
    "\n",
    "from data_juicer.utils.constant import Fields\n",
    "# NOTE: use a new definition above\n",
    "# from data_juicer.utils.constant import StatsKeys\n",
    "from data_juicer.ops.base_op import OPERATORS, Filter\n",
    "\n",
    "\n",
    "@OPERATORS.register_module('your_text_length_filter')\n",
    "class YourTextLengthFilter(Filter):\n",
    "    \"\"\"Filter to keep samples with total text length within a specific\n",
    "    range.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                min_len: PositiveInt = 10,\n",
    "                max_len: PositiveInt = sys.maxsize,\n",
    "                *args,\n",
    "                **kwargs):\n",
    "        \"\"\"\n",
    "        Initialization method.\n",
    "\n",
    "        :param min_len: The min text length in the filtering. samples\n",
    "            will be filtered if their text length is below this\n",
    "            parameter.\n",
    "        :param max_len: The max text length in the filtering. samples\n",
    "            will be filtered if their text length exceeds this\n",
    "            parameter.\n",
    "        :param args: extra args\n",
    "        :param kwargs: extra args\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.min_len = min_len\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def compute_stats(self, sample):\n",
    "        # check if it's computed already\n",
    "        if StatsKeys.text_len in sample[Fields.stats]:\n",
    "            return sample\n",
    "\n",
    "        sample[Fields.stats][StatsKeys.text_len] = len(sample[self.text_key])\n",
    "        return sample\n",
    "\n",
    "    def process(self, sample):\n",
    "        if self.min_len <= sample[Fields.stats][StatsKeys.text_len] <= self.max_len:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After implemention, add it to the OP dictionary in the `__init__.py` file in `data_juicer/ops/filter/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    from . import your_text_length_filter\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for Your Operator\n",
    "\n",
    "It's better to add corresponding tests for your own OPs. For `YourTextLengthFilter` above, you would like to add `test_text_length_filter.py` into `tests/ops/filter/` directory as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# NOTE: use a new definition above\n",
    "# from data_juicer.ops.filter.text_length_filter import YourTextLengthFilter\n",
    "from data_juicer.utils.constant import Fields\n",
    "from data_juicer.utils.unittest_utils import DataJuicerTestCaseBase\n",
    "\n",
    "class YourTextLengthFilterTest(DataJuicerTestCaseBase):\n",
    "\n",
    "    def _run_text_length_filter(self, dataset: Dataset, target_list, op):\n",
    "        if Fields.stats not in dataset.features:\n",
    "            dataset = dataset.add_column(name=Fields.stats,\n",
    "                                         column=[{}] * dataset.num_rows)\n",
    "        dataset = dataset.map(op.compute_stats)\n",
    "        dataset = dataset.filter(op.process)\n",
    "        dataset = dataset.select_columns(column_names=['text'])\n",
    "        res_list = dataset.to_list()\n",
    "        print(res_list)\n",
    "        self.assertEqual(res_list, target_list)\n",
    "\n",
    "    def test_case1(self):\n",
    "\n",
    "        ds_list = [{\n",
    "            'text': '123'\n",
    "        }, {\n",
    "            'text': '12345'\n",
    "        }, {\n",
    "            'text': '1234567'\n",
    "        }]\n",
    "        tgt_list = [{\n",
    "            'text': '12345'\n",
    "        }]\n",
    "        dataset = Dataset.from_list(ds_list)\n",
    "        op = YourTextLengthFilter(min_len=4, max_len=6)\n",
    "        self._run_text_length_filter(dataset, tgt_list, op)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # NOTE: for run in Jupyter\n",
    "    # unittest.main()\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Supports for Your Operator\n",
    "\n",
    "- If Hugging Face models are used within an operator, you might want to leverage GPU acceleration. To achieve this, declare `self._accelerator = 'cuda'` in the constructor, and ensure that `compute_stats` and `process` methods accept an additional positional argument `rank`.\n",
    "```python\n",
    "    # ... (same as above)\n",
    "    from data_juicer.utils.model_utils import get_model, prepare_model\n",
    "\n",
    "    @OPERATORS.register_module('your_text_length_filter_with_cuda')\n",
    "    class YourTextLengthFilterWithCuda(Filter):\n",
    "        def __init__(self,\n",
    "                    min_len: PositiveInt = 10,\n",
    "                    max_len: PositiveInt = sys.maxsize,\n",
    "                    *args,\n",
    "                    **kwargs):\n",
    "            # ... (same as above)\n",
    "            self._accelerator = 'cuda'\n",
    "\n",
    "        def compute_stats(self, sample, rank=None):\n",
    "            # ... (some codes)\n",
    "            if rank:\n",
    "                model.to(f'cuda:{rank}')\n",
    "            \n",
    "        def process(self, sample, rank=None):\n",
    "            # ... (same as above)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If an operator takes one sample as input and produces multiple samples, the input and output need to be batched together by declaring `self._batched_op = True`. This feature is currently only supported by mapper operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from jsonargparse.typing import PositiveInt\n",
    "\n",
    "from data_juicer.ops.base_op import OPERATORS, Mapper\n",
    "\n",
    "\n",
    "@OPERATORS.register_module('your_batch_mapper')\n",
    "class YourBatchMapper(Mapper):\n",
    "    \"\"\"A mapper operator processing batched samples.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                *args,\n",
    "                **kwargs):\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._batched_op = True\n",
    "\n",
    "    def process(self, samples):\n",
    "        # reconstruct samples from \"dict of lists\" to \"list of dicts\"\n",
    "        reconstructed_samples = []\n",
    "        for i in range(len(samples[self.text_key])):\n",
    "            reconstructed_samples.append(\n",
    "                {key: samples[key][i]\n",
    "                 for key in samples})\n",
    "\n",
    "        # duplicate\n",
    "        samples_after_generation = []\n",
    "        for sample in reconstructed_samples:\n",
    "            samples_after_generation.extend([sample, sample])\n",
    "\n",
    "        # reconstruct samples from \"list of dicts\" to \"dict of lists\"\n",
    "        keys = samples_after_generation[0].keys()\n",
    "        res_samples = {}\n",
    "        for key in keys:\n",
    "            res_samples[key] = [s[key] for s in samples_after_generation]\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_juicer.core.data import NestedDataset\n",
    "\n",
    "ds_list = [{\n",
    "            'text': '123'\n",
    "        }, {\n",
    "            'text': '12345'\n",
    "        }, {\n",
    "            'text': '1234567'\n",
    "        }]\n",
    "print('unbatched samples', ds_list)\n",
    "dataset = NestedDataset.from_list(ds_list)\n",
    "op = YourBatchMapper()\n",
    "dataset = dataset.map(op.process)\n",
    "print(dataset.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Call `transfer_filename` and `add_suffix_to_filename` to get unique paths for saving of extra datas, such as images and videos, to prevent data coverage and ensure process security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_juicer.utils.file_utils import add_suffix_to_filename, transfer_filename\n",
    "from data_juicer.ops.op_fusion import LOADED_VIDEOS\n",
    "from data_juicer.ops.base_op import OPERATORS, Mapper\n",
    "# ... (import some other libraries)\n",
    "\n",
    "OP_NAME = 'your_video_split_by_key_frame_mapper'\n",
    "@OPERATORS.register_module(OP_NAME)\n",
    "@LOADED_VIDEOS.register_module(OP_NAME)\n",
    "class YourVideoSplitByKeyFrameMapper(Mapper):\n",
    "    def __init__(self,\n",
    "             # ... (OP parameters)\n",
    "             split_num = 1,\n",
    "             *args,\n",
    "             **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._init_parameters = self.remove_extra_parameters(locals())\n",
    "        print(f'init parameters: {self._init_parameters}')\n",
    "        self.split_num = split_num\n",
    "\n",
    "    def process(self, sample):\n",
    "        # ... (some codes)\n",
    "        original_video_path = sample['videos'][0]\n",
    "        base_video_path = transfer_filename(\n",
    "                    original_video_path, OP_NAME, **self._init_parameters)\n",
    "        print(f'base path: {base_video_path}')\n",
    "        for count in range(self.split_num):\n",
    "            split_video_path = add_suffix_to_filename(base_video_path,  f'_{count}')\n",
    "            print(f'split {count} path: {split_video_path}')\n",
    "        # ... (some codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {'videos': ['./video.mp4']}\n",
    "print('------ 2 splits ------')\n",
    "op = YourVideoSplitByKeyFrameMapper(split_num=2)\n",
    "op.process(sample)\n",
    "print('------ 3 splits ------')\n",
    "op = YourVideoSplitByKeyFrameMapper(split_num=3)\n",
    "op.process(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish the Documents\n",
    "\n",
    " In order to facilitate the use of other users, we also need to update this new operator information to the corresponding documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `configs/config_all.yaml`: this complete config file contains a list of all OPs and their arguments, serving as an\n",
    "   important document for users to refer to all available OPs. Therefore, after adding the new OP, we need to add it to the process\n",
    "   list (grouped by the OP type and sorted in alphabetical order):\n",
    "   \n",
    "   ```yaml\n",
    "   ...\n",
    "   - your_text_length_filter:                                # filter text with length out of specific range\n",
    "       min_len: 10                                             # the min length of filter range\n",
    "       max_len: 10000                                          # the max length of filter range\n",
    "   ...\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `docs/Operators.md`: this doc maintains categorized lists of available OPs. We can add the information of new OP to the list\n",
    "   of corresponding type of OPs (sorted in alphabetical order). At the same time, in the Overview section at the top of this doc,\n",
    "   we also need to update the number of OPs for the corresponding OP type:\n",
    "\n",
    "   ```markdown\n",
    "   ## Overview\n",
    "   ...\n",
    "   | [ Filter ]( #filter )             |   21 (+1 HERE)   | Filters out low-quality samples                 |\n",
    "   ...\n",
    "   ## Filter <a name=\"filter\"/>\n",
    "   ...\n",
    "   | suffix_filter                  | General | en, zh | Keeps samples with specified suffixes                                                      |\n",
    "   | your_text_length_filter        | General | en, zh | Keeps samples with total text length within the specified range                            |\n",
    "   | token_num_filter               | General | en, zh | Keeps samples with token count within the specified range                                  |\n",
    "   ...\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `docs/Operators_ZH.md`: this doc is the Chinese version of the doc in 6.ii, so we need to update the Chinese content at\n",
    "   the same positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `docs/sphinx_doc/source/data_juicer.ops.{filter | mapper | deduplicator | selector}.rst`: this doc is the index of API reference. When the operator file name is modified or an operator file is added or deleted, the corresponding entries in the file need to be updated accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Style\n",
    "\n",
    "We define our styles in `.pre-commit-config.yaml`. Before committing,\n",
    "please install `pre-commit` tool to check and modify accordingly:\n",
    "\n",
    "```shell\n",
    "# ===========install pre-commit tool===========\n",
    "pip install pre-commit\n",
    "\n",
    "cd <path_to_data_juicer>\n",
    "# install pre-commit script for data_juicer\n",
    "pre-commit install\n",
    "\n",
    "\n",
    "# ===========check all files===========\n",
    "git add .\n",
    "pre-commit run --all-files\n",
    "\n",
    "# commit after all checking are passed\n",
    "git commit -m \"xxxx\"\n",
    "```\n",
    "\n",
    "**Note**: We have configured pre-commit checks in github workflow. If this \n",
    "check in your PR fails, please locally ① ensure that the relevant \n",
    "dependencies of pre-commit are consistent with the project configuration \n",
    "(which can be completed through `pre-commit clean` and `pre-commit install`); \n",
    "and ② execute `pre-commit run --all-files` before push.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Make your OP fusible\n",
    "\n",
    "- If the calculation process of some intermediate variables in the new OP is reused in other existing OPs, this new OP can be\n",
    "added to the fusible OPs to accelerate the whole data processing with OP fusion technology. (e.g. both the `word_num_filter`\n",
    "and `word_repetition_filter` need to split the input text into words)\n",
    "- When opening OP fusion, these reused calculation processes and intermediate variables can be shared in the `context` between\n",
    "OPs, thus reducing repeated calculations.\n",
    "- OPs that contain common intermediate variables can be fused in OP fusion through the following steps:\n",
    "\n",
    "1. (Optional) If a new intermediate variable is generated in the new OP, we need to add this new intermediate variable name to \n",
    "the `InterVars` class in `utils/constant.py`. In general, we need to add a prefix `DEFAULT_PREFIX` before the name.\n",
    "\n",
    "```python\n",
    "    class InterVars(object):\n",
    "        # text\n",
    "        lines = DEFAULT_PREFIX + 'lines'\n",
    "        words = DEFAULT_PREFIX + 'words'  # add the new intermediate variable here\n",
    "        ...\n",
    "```\n",
    "\n",
    "2. (Optional) We need to define a registry group in `ops/op_fusion.py` for the new intermediate variable in the 1st step, and add\n",
    "this registry group to the registry group list that stores all groups of intermediate variables. This facilitates the OP Fusion module\n",
    "to track OPs involving these intermediate variables.\n",
    "\n",
    "```python\n",
    "    ...\n",
    "    # Type of intermediate vars\n",
    "    # text\n",
    "    INTER_LINES = Registry(InterVars.lines)\n",
    "    INTER_WORDS = Registry(InterVars.words)  # define registry group for the new intermediate variable\n",
    "\n",
    "    # images\n",
    "    LOADED_IMAGES = Registry(InterVars.loaded_images)\n",
    "\n",
    "    # all\n",
    "    ALL_INTER_VARS = [INTER_LINES, INTER_WORDS, LOADED_IMAGES]  # and add it to the registry group list\n",
    "    ...\n",
    "```\n",
    "\n",
    "3. Before the OP class definition that involves the intermediate variable, register this OP in the registry group corresponding\n",
    "to this intermediate variable, indicating that the intermediate variable may be calculated and used in this OP.\n",
    "\n",
    "```python\n",
    "    ...\n",
    "    @OPERATORS.register_module(OP_NAME)\n",
    "    @INTER_WORDS.register_module(OP_NAME)  # register this new OP into the registry group\n",
    "    class WordNumFilter(Filter):\n",
    "    ...\n",
    "```\n",
    "\n",
    "4. In the calculation process of this intermediate variable of the new OP, we can modify the calculation logic to:\n",
    "   1. If the argument `context` is True, it means the OP fusion is opening, so we get the value of this intermediate variable \n",
    "   from `context` first, which has been calculated by the previous OPs.\n",
    "   2. If this intermediate variable doesn't exist in the `context`, it means it's the first time to calculate this variable in this\n",
    "   OP, so we need to define a unique key and use it to store the intermediate variable in the `context` for subsequent OPs after\n",
    "   it's calculated by this new OP.\n",
    "   3. If the argument `context` is False, just follow the normal calculation process.\n",
    "\n",
    "```python\n",
    "    # before modification\n",
    "    ...\n",
    "    tokenizer = get_model(self.model_key)\n",
    "    words = get_words_from_document(\n",
    "        sample[self.text_key],\n",
    "        token_func=tokenizer.encode_as_pieces if tokenizer else None)\n",
    "    ...        \n",
    "\n",
    "    # after modification\n",
    "    ...\n",
    "    words_key = f'{InterVars.words}-{self.model_key}'\n",
    "    if context and words_key in sample[Fields.context]:\n",
    "        # get the value of intermediate variable from context directly\n",
    "        words = sample[Fields.context][words_key]\n",
    "    else:\n",
    "        # normal calculation process\n",
    "        tokenizer = get_model(self.model_key)\n",
    "        words = get_words_from_document(\n",
    "            sample[self.text_key],\n",
    "            token_func=tokenizer.encode_as_pieces if tokenizer else None)\n",
    "        if context:\n",
    "            # After calculating the intermediate variable for the first time,\n",
    "            # store it in the context for subsequent OPs.\n",
    "            sample[Fields.context][words_key] = words\n",
    "    ...\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
